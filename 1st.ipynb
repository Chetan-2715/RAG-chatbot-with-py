{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84631552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama  \n",
    "'''\n",
    "This imports the ChatOllama class from LangChain’s Ollama integration.\n",
    "Ollama = a local LLM runner\n",
    "'''\n",
    "\n",
    "base_url = \"http://localhost:11434\"  # This is where your Ollama server is running. By default, Ollama runs locally at port 11434.\n",
    "model = \"gemma3:1b\"\n",
    "\n",
    "llm = ChatOllama(    # Creates an LLM instance\n",
    "    base_url = base_url,\n",
    "    model = model,\n",
    "    temperature = 0.7,   #  Controls creativity.Low = focused, deterministic ; High = more creative, diverse\n",
    "    num_predict = 200,  # How many tokens the model should try to predict.\n",
    "    max_tokens = 100   #  Hard cap on output length (won’t go beyond 100 tokens).\n",
    ")\n",
    "\n",
    "#  The whoole above thing is saying : Load Gemma3 (1B) model from Ollama, and configure how it should answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. \n",
      "\n",
      "It’s a common misconception that Paris is the only capital, but it’s the *official* capital. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the capital of France?\")  # Sends your query (prompt) to the LLM.\n",
    "print(response.content)    #  Extracts the text part of the response (instead of the whole metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32edbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gemma3:1b',\n",
       " 'created_at': '2025-08-21T17:56:15.203873Z',\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'total_duration': 1179711600,\n",
       " 'load_duration': 139985200,\n",
       " 'prompt_eval_count': 16,\n",
       " 'prompt_eval_duration': 117290600,\n",
       " 'eval_count': 36,\n",
       " 'eval_duration': 921932200,\n",
       " 'model_name': 'gemma3:1b'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata  \n",
    "\"\"\"\n",
    "Here, instead of the text, you’re checking the metadata the model returned.\n",
    "Example: token usage, timings, model name, etc.\n",
    "This is useful for debugging and optimization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74da7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! I’m doing well, thank you for asking. As an AI, I don’t really experience feelings the way humans do, but I’m functioning properly and ready to help you with whatever you need. \n",
      "\n",
      "How about you? How’s your day going?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"hi, how are you?\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-1 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
